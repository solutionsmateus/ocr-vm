import os
import glob
import zipfile
import time
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import pandas as pd
import io

# --- 1. Configura√ß√£o Inicial ---
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
artifact_folder = os.environ.get("ARTIFACT_FOLDER", "./workflow-github-action")

if not api_key:
    print("Erro: A 'GEMINI_API_KEY' n√£o foi encontrada.")
    print("Por favor, crie um arquivo '.env' com sua chave.")
    exit()

try:
    genai.configure(api_key=api_key)
except Exception as e:
    print(f"Erro ao configurar a API Gemini: {e}")
    exit()

safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

model = genai.GenerativeModel(
    model_name='gemini-flash-latest', 
    safety_settings=safety_settings
)

PROMPT_TEXT = """
Transforme o PDF/PNG/JPEG em tabela Markdown (para copiar no Excel) e XLSX, usando esta ordem EXATA de colunas:

Empresa, Data, Data In√≠cio, Data Fim, Campanha, Categoria do Produto, Produto, Medida, Quantidade, Pre√ßo, App, Loja, Cidade, Estado.

‚úÖ REGRAS OBRIGAT√ìRIAS
‚úÖ EMPRESA

Nunca substituir supermercado pela campanha.

Permitir apenas estes valores:

Assa√≠ Atacadista

Atacad√£o

Cometa Supermercados

Frangol√¢ndia

GBarbosa

Atakarejo

Novo Atakarejo

Se o encarte tiver outra empresa ‚Üí deixar em branco (nunca inventar).

‚úÖ DATA

Data = ‚ÄúData In√≠cio - Data Fim‚Äù (DD/MM/AAAA)

Data In√≠cio e Data Fim tamb√©m devem aparecer separadamente.

‚úÖ CAMPANHA

Formato: Nome da campanha + dia da oferta + Estado

Nunca colocar campanha dentro da coluna Empresa.

‚úÖ PRODUTO

Sem refer√™ncia/c√≥digo (ex.: ‚Äúcx‚Äù, ‚Äúref‚Äù, SKU, c√≥digo interno)

Se o nome estiver incompleto, n√£o inventar.

‚úÖ MEDIDA

Detectar apenas as unidades:

g, mg, kg, litro, cm, metro
(se n√£o houver medida, deixar vazio)

‚úÖ QUANTIDADE

1 quando for item unit√°rio.

Se for pack/kit/leve X/caixa ‚Üí usar o n√∫mero total de unidades.

‚úÖ LOJA (IMPORTANTE)

Deve conter todas as CIDADES onde o encarte atua

Separar m√∫ltiplas cidades com "; "

Sempre com acentua√ß√£o e ortografia correta:

Primeira letra mai√∫scula, restante min√∫scula

Ex.: S√£o Lu√≠s; Imperatriz; Bacabal; Macei√≥; Arapiraca

‚úÖ CIDADE (IMPORTANTE)

Deve ser apenas a cidade padr√£o do estado, mesmo que haja v√°rias lojas:

ESTADO (MAI√öSCULO)	Cidade padr√£o (capitalizada corretamente)
MARANH√ÉO	S√£o Lu√≠s
CEAR√Å	Fortaleza
PAR√Å	Bel√©m
PERNAMBUCO	Recife
ALAGOAS	Macei√≥
SERGIPE	Aracaju
BAHIA	Salvador
PIAU√ç	Teresina
PARA√çBA	Jo√£o Pessoa
‚úÖ ESTADO

Nome por extenso e EM MAI√öSCULAS

Ex.: MARANH√ÉO, CEAR√Å, PAR√Å, PERNAMBUCO, ALAGOAS, SERGIPE, BAHIA‚Ä¶

‚úÖ PADR√ïES GERAIS

Nunca duplicar itens

N√£o inventar dados ‚Äî se n√£o estiver no encarte, deixar em branco

Corrigir acentos, erros de OCR e n√∫meros

Extrair somente o que existe na imagem

üõë **AVISO CR√çTICO**: N√ÉO utilize o caractere PIPE (|) dentro de NENHUM campo de texto ou dado. Se precisar de separador, use v√≠rgula ou ponto-e-v√≠rgula.
"""

# Extens√µes de arquivo 
VALID_EXTENSIONS = ('.jpeg', '.jpg', '.png', '.pdf')
BATCH_SIZE = 1
all_markdown_results = []
all_dataframes = [] 

def parse_markdown_table(markdown_text):
    """
    Analisa a string de tabela Markdown e a converte em um DataFrame do pandas.
    Adiciona resili√™ncia contra problemas de tokeniza√ß√£o causados por pipes internos.
    """
    # Nomes EXATOS das 14 colunas
    COLUMNS = [
        "Empresa", "Data", "Data In√≠cio", "Data Fim", "Campanha", 
        "Categoria do Produto", "Produto", "Medida", "Quantidade", 
        "Pre√ßo", "App", "Loja", "Cidade", "Estado"
    ]
    
    try:
        # Divide o texto em linhas
        lines = markdown_text.strip().split('\n')
        
        # Filtra as linhas:
        # 1. Remove a primeira linha (cabe√ßalho) e a segunda linha (separador Markdown |---|)
        # 2. Mant√©m apenas as linhas que parecem ser dados (cont√©m o separador |)
        data_lines = [line for line in lines[2:] if line.strip().startswith('|')]
        
        # Junta as linhas de dados novamente em uma √∫nica string
        cleaned_data = '\n'.join(data_lines)
        data = io.StringIO(cleaned_data)
        
        # Tenta ler a tabela. Usamos 'header=None' e 'engine='python'' para maior toler√¢ncia.
        df = pd.read_csv(
            data, 
            sep='|', 
            skipinitialspace=True, 
            header=None,
            on_bad_lines='warn', # Avisa sobre linhas problem√°ticas, mas tenta continuar
            engine='python' 
        )
        
        # Limpeza p√≥s-leitura
        # Remove a primeira e a √∫ltima coluna (vazias devido ao formato |col1|col2|)
        df = df.iloc[:, 1:-1]
        
        # Define os nomes das colunas
        if df.shape[1] == len(COLUMNS):
            df.columns = COLUMNS
        else:
            print(f"AVISO CR√çTICO: Colunas esperadas ({len(COLUMNS)}) != Colunas detectadas ({df.shape[1]}). Aplicando reajuste for√ßado.")
            # Se o n√∫mero de colunas n√£o bater, tentamos prosseguir descartando colunas extras
            if df.shape[1] > len(COLUMNS):
                df = df.iloc[:, :len(COLUMNS)]
                df.columns = COLUMNS
                print("Reajuste for√ßado aplicado: colunas extras descartadas.")
            else:
                 # Se houver menos colunas, preenchemos com NaN no final
                missing_cols = len(COLUMNS) - df.shape[1]
                for i in range(missing_cols):
                    df[f'COL_MISSING_{i}'] = None
                df.columns = COLUMNS
                print("Reajuste for√ßado aplicado: colunas faltantes adicionadas.")
            
        # Remove linhas que s√£o todas NaN (podem ser linhas vazias residuais)
        df.dropna(how='all', inplace=True)
        
        return df
        
    except Exception as e:
        print(f"AVISO: N√£o foi poss√≠vel converter a tabela Markdown em DataFrame. Erro: {e}")
        return None

def save_dataframes_to_excel(dataframes, output_filename="gemini_resultados_compilados.xlsx"):
    """
    Compila todos os DataFrames em um √∫nico arquivo XLSX.
    """
    if not dataframes:
        print("Nenhum DataFrame para salvar.")
        return

    try:
        # Concatenar todos os DataFrames em um √∫nico
        final_df = pd.concat(dataframes, ignore_index=True)
        
        # Salva em XLSX
        final_df.to_excel(output_filename, index=False, engine='openpyxl')
        
        print(f"SUCESSO!")
        print(f"Todos os arquivos foram processados.")
        print(f"Resultado salvo em: {output_filename}")
    except Exception as e:
        print(f"ERRO ao salvar o arquivo final XLSX: {e}")

def process_files():
    """
    Fun√ß√£o principal para executar todo o fluxo de trabalho.
    """
    
    # 2. Extrair todos os Zips
    print(f"Procurando por arquivos .zip em {artifact_folder}...")
    zip_pattern = os.path.join(artifact_folder, "**", "*.zip")
    zip_files = glob.glob(zip_pattern, recursive=True)

    if not zip_files:
        print("Nenhum arquivo .zip encontrado. Verificando arquivos existentes...")
    else:
        print(f"Encontrados {len(zip_files)} arquivos .zip. Extraindo...")
        for zip_path in zip_files:
            try:
                extract_directory = os.path.dirname(zip_path)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_directory)
                print(f"Extra√≠do: {zip_path} -> {extract_directory}")
            except zipfile.BadZipFile:
                print(f"Erro: {zip_path} n√£o √© um arquivo zip v√°lido ou est√° corrompido.")
            except Exception as e:
                print(f"Erro ao extrair {zip_path}: {e}")
        print("Extra√ß√£o de Zips conclu√≠da.\n")

    print("Iniciando varredura das pastas de supermercados...")
    
    for root, dirs, files in os.walk(artifact_folder, topdown=False):
        
        if not dirs and files and root != artifact_folder:
            
            file_paths_to_process = [
                os.path.join(root, f) for f in files if f.lower().endswith(VALID_EXTENSIONS)
            ]

            if not file_paths_to_process:
                continue 

            print(f"--- Processando Diret√≥rio: {root} ---")
            print(f"Encontrados {len(file_paths_to_process)} arquivos v√°lidos.")

            for i in range(0, len(file_paths_to_process), BATCH_SIZE):
                batch_paths = file_paths_to_process[i : i + BATCH_SIZE]
                print(f"  Processando lote {i//BATCH_SIZE + 1} ({len(batch_paths)} arquivos)...")
                time.sleep(1)

                uploaded_files = []
                prompt_payload = []

                for path in batch_paths:
                    try:
                        print(f"    Subindo arquivo: {os.path.basename(path)}") 
                        file = genai.upload_file(path=path)
                        uploaded_files.append(file)
                        time.sleep(1)
                    except Exception as e:
                        print(f"    ERRO ao subir {path}: {e}")
                
                if not uploaded_files:
                    print("    Nenhum arquivo foi upado com sucesso neste lote. Pulando.")
                    continue

                prompt_payload = [
                    f"{len(uploaded_files)} arquivos anexados.",
                    PROMPT_TEXT
                ] + uploaded_files

                try:
                    print(f"    Enviando {len(uploaded_files)} arquivos para o Gemini...")
                    response = model.generate_content(prompt_payload)
                    
                    # üí° NOVO: Converte a resposta Markdown para DataFrame e armazena
                    df = parse_markdown_table(response.text)
                    if df is not None:
                        all_dataframes.append(df)
                        print(f"    Resposta recebida e convertida em DataFrame.")
                    else:
                        # Se a convers√£o falhar, ainda tentamos printar a resposta bruta para debug
                        print(f"    Resposta bruta do Gemini (pode conter erro de formata√ß√£o):")
                        print("--- IN√çCIO DA RESPOSTA BRUTA ---")
                        print(response.text)
                        print("--- FIM DA RESPOSTA BRUTA ---")
                        print(f"    Resposta recebida, mas falhou na convers√£o para DataFrame.")
                    
                except Exception as e:
                    print(f"    ERRO ao chamar a API Gemini: {e}")
                
                finally:
                    print("    Limpando arquivos do servidor Gemini...")
                    for file in uploaded_files:
                        try:
                            time.sleep(1) # Pausa para evitar limite de taxa
                            genai.delete_file(file.name)
                        except Exception as e:
                            print(f"    Erro ao deletar arquivo {file.name}: {e}")
            
            print(f"--- Diret√≥rio {root} conclu√≠do ---\n")

    if not all_dataframes:
        print("Nenhum resultado foi gerado pela API ou convertido para DataFrame.")
    else:
        # üí° NOVO: Chamada para salvar em XLSX
        save_dataframes_to_excel(all_dataframes)


if __name__ == "__main__":
    # Verifica√ß√£o de depend√™ncias
    try:
        import pandas as pd
        import openpyxl # openpyxl √© o motor padr√£o para escrita de XLSX pelo pandas
    except ImportError:
        print("\n--- DEPEND√äNCIA FALTANDO ---")
        print("Para salvar em XLSX, voc√™ precisa instalar pandas e openpyxl.")
        print("Execute o comando:")
        print("pip install pandas openpyxl")
        exit()

    try:
        process_files()
    except Exception as e:
        print(f"Um erro inesperado e fatal ocorreu: {e}")