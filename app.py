import os
import glob
import zipfile
import time
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from google.genai.errors import APIError 
from google.genai.types import HarmCategory, HarmBlockThreshold, GenerateContentConfig, SafetySetting 
import pandas as pd
import io
import re 
from concurrent.futures import ThreadPoolExecutor, as_completed 
from itertools import cycle # Importa cycle para rotacionar as chaves

load_dotenv()

# --- Configura√ß√£o de Chaves e Vari√°veis de Ambiente ---
API_KEY_LIST = []
key_primary = os.environ.get("GEMINI_API_KEY")
if key_primary:
    API_KEY_LIST.append(key_primary)

key_backup_1 = os.environ.get("GEMINI_API_KEY_BACKUP_01")
if key_backup_1:
    API_KEY_LIST.append(key_backup_1)

key_backup_2 = os.environ.get("GEMINI_API_KEY_BACKUP_02")
if key_backup_2:
    API_KEY_LIST.append(key_backup_2)

if not API_KEY_LIST:
    print("Erro: Nenhuma chave de API (GEMINI_API_KEY_PRIMARY ou BACKUP) foi encontrada nas vari√°veis de ambiente.")
    print("Por favor, verifique se os secrets est√£o configurados no GitHub e injetados no YAML.")
    exit()

artifact_folder = os.environ.get("ARTIFACT_FOLDER", "./workflow-github-action")
# --- Fim da Configura√ß√£o de Chaves e Vari√°veis de Ambiente ---


safety_settings_list = [
    SafetySetting(
        category=HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold=HarmBlockThreshold.BLOCK_NONE
    ),
    SafetySetting(
        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold=HarmBlockThreshold.BLOCK_NONE
    ),
    SafetySetting(
        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold=HarmBlockThreshold.BLOCK_NONE
    ),
    SafetySetting(
        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=HarmBlockThreshold.BLOCK_NONE
    ),
]

MODEL_NAME = 'gemini-2.5-flash' 

# --- PROMPT INALTERADO ---
PROMPT_TEXT = """
Transforme o PDF/PNG/JPEG em tabela Markdown (para copiar no Excel) e XLSX, usando esta ordem EXATA de colunas:

Empresa, Data, Data In√≠cio, Data Fim, Campanha, Categoria do Produto, Produto, Medida, Quantidade, Pre√ßo, App, Loja, Cidade, Estado.

‚úÖ REGRAS OBRIGAT√ìRIAS
‚úÖ EMPRESA

Nunca substituir supermercado pela campanha.

Permitir apenas estes valores:

Assa√≠ Atacadista

Atacad√£o

Cometa Supermercados

Frangol√¢ndia

GBarbosa

Atakarejo

Novo Atakarejo

Se o encarte tiver outra empresa ‚Üí deixar em branco (never inventar).

‚úÖ DATA

Data = ‚ÄúData In√≠cio - Data Fim‚Äù (DD/MM/AAAA)

Data In√≠cio e Data Fim tamb√©m devem aparecer separadamente.

‚úÖ CAMPANHA

Formato: Nome da campanha + Data do Encarte + Estado

Nunca colocar campanha dentro da coluna Empresa.

‚úÖ PRODUTO

Sem refer√™ncia/c√≥digo (ex.: ‚Äúcx‚Äù, ‚Äúref‚Äù, SKU, c√≥digo interno)

Se o nome estiver incompleto, n√£o inventar.

Preciso de todas as informa√ß√µes que estiverem na imagem de cada produto.

‚úÖ MEDIDA

Detectar apenas as unidades:

g, mg, kg, litro, cm, metro
(se n√£o houver medida, deixar vazio)

‚úÖ QUANTIDADE

1 quando for item unit√°rio.

Se for pack/kit/leve X/caixa ‚Üí usar o n√∫mero total de unidades.

‚úÖ LOJA (IMPORTANTE)

Deve conter todas as CIDADES onde o encarte atua

Separar m√∫ltiplas cidades com "; "

Sempre com acentua√ß√£o e ortografia correta:

Primeira letra mai√∫scula, restante min√∫scula

Ex.: S√£o Lu√≠s; Imperatriz; Bacabal; Macei√≥; Arapiraca

‚úÖ CIDADE (IMPORTANTE)

Deve ser apenas a cidade padr√£o do estado, mesmo que haja v√°rias lojas:

ESTADO (MAI√öSCULO)	Cidade padr√£o (capitalizada corretamente)
MARANH√ÉO	S√£o Lu√≠s
CEAR√Å	Fortaleza
PAR√Å	Bel√©m
PERNAMBUCO	Recife
ALAGOAS	Macei√≥
SERGIPE	Aracaju
BAHIA	Salvador
PIAU√ç	Teresina
PARA√çBA	Jo√£o Pessoa
‚úÖ ESTADO

Nome por extenso e EM MAI√öSCULAS

Ex.: MARANH√ÉO, CEAR√Å, PAR√Å, PERNAMBUCO, ALAGOAS, SERGIPE, BAHIA‚Ä¶

‚úÖ PADR√ïES GERAIS

Nunca duplicar itens

N√£o inventar dados ‚Äî se n√£o estiver no encarte, deixar em branco

Corrigir acentos, erros de OCR e n√∫meros

Extrair somente o que existe na imagem

Siga estes detalhes minunciosamente: 
DETALHE 1: : QUANDO FOR ENCARTES DO COMETA SUPERMERCADOS, A CIDADE E LOJA SEMPRE V√ÉO SER ‚ÄúFORTALEZA‚Äù E O ESTADO: CEAR√Å

DETALHE 2: QUANDO FOR ENCARTES DO NOVO ATACAREJO, A LOJA SEMPRE VAI SER "Olinda", A CIDADE: "Recife" E O ESTADO: "PERNAMBUCO"

DETALHE 3: LEIA A DESCRI√á√ÉO COMPLETA DOS PRODUTOS DOS ENCARTES DE TODOS OS SUPERMERCADOS, EU PRECISO DE TODAS AS INFORMA√á√ïES CORRETAS NOS SEUS LUGARES DEVIDOS DE ACORDO COM AS CATEGORIAS CITADAS ACIMA.

**AVISO CR√çTICO**: N√ÉO utilize o caractere PIPE (|) dentro de NENHUM campo de texto ou dado. Se precisar de separador, use v√≠rgula ou ponto-e-v√≠rgula.
"""
# --- FIM DO PROMPT INALTERADO ---


VALID_EXTENSIONS = ('.jpeg', '.jpg', '.png', '.pdf')
BATCH_SIZE = 1 
MAX_THREADS = 8 

all_markdown_results = []
all_dataframes = [] 


def parse_markdown_table(markdown_text):
    COLUMNS = [
        "Empresa", "Data", "Data In√≠cio", "Data Fim", "Campanha", 
        "Categoria do Produto", "Produto", "Medida", "Quantidade", 
        "Pre√ßo", "App", "Loja", "Cidade", "Estado"
    ]
    
    try:
        lines = markdown_text.strip().split('\n')
        # Filtra linhas que come√ßam com '|' e exclui as duas primeiras linhas (header e separador)
        data_lines = [line for line in lines if line.strip().startswith('|')][2:]
        
        cleaned_data = '\n'.join(data_lines)
        data = io.StringIO(cleaned_data)
        
        # Leitura robusta da tabela Markdown
        df = pd.read_csv(
            data, 
            sep='|', 
            skipinitialspace=True, 
            header=None,
            on_bad_lines='warn',
            engine='python' 
        )
        
        # Remove a primeira e a √∫ltima coluna (que s√£o separadores vazios)
        if df.shape[1] >= 2:
            df = df.iloc[:, 1:-1]
        
        # Tratamento de colunas faltantes/extras
        if df.shape[1] == len(COLUMNS):
            df.columns = COLUMNS
        else:
            print(f"AVISO CR√çTICO: Colunas esperadas ({len(COLUMNS)}) != Colunas detectadas ({df.shape[1]}). Aplicando reajuste for√ßado.")
            if df.shape[1] > len(COLUMNS):
                df = df.iloc[:, :len(COLUMNS)]
                df.columns = COLUMNS
                print("Reajuste for√ßado aplicado: colunas extras descartadas.")
            else:
                missing_cols = len(COLUMNS) - df.shape[1]
                # Adiciona colunas faltantes com None
                for i in range(missing_cols):
                    df[f'COL_MISSING_{i}'] = None
                df.columns = COLUMNS
                print("Reajuste for√ßado aplicado: colunas faltantes adicionadas.")
            
        df.dropna(how='all', inplace=True)
        
        return df
        
    except Exception as e:
        print(f"AVISO: N√£o foi poss√≠vel converter a tabela Markdown em DataFrame. Erro: {e}")
        return None

def save_dataframes_to_excel(dataframes, output_filename="gemini_resultados_compilados.xlsx"):
    if not dataframes:
        print("Nenhum DataFrame para salvar.")
        return

    try:
        final_df = pd.concat(dataframes, ignore_index=True)
        
        final_df.to_excel(output_filename, index=False, engine='openpyxl')
        
        print(f"SUCESSO!")
        print(f"Todos os arquivos foram processados.")
        print(f"Resultado salvo em: {output_filename}")
    except Exception as e:
        print(f"ERRO ao salvar o arquivo final XLSX: {e}")


# üí° FUN√á√ÉO OTIMIZADA: Tenta a sequ√™ncia UPLOAD -> GENERATE -> DELETE com v√°rias chaves em caso de falha.
def process_single_file(file_path, key_iterator):
    uploaded_file = None
    
    # üîÑ Loop de Failover dentro da Thread
    for i in range(len(API_KEY_LIST)):
        api_key = next(key_iterator)
        key_name = f"Chave de Failover #{i + 1}"
        
        try:
            # 1. Configura o cliente para esta tentativa
            client = genai.Client(api_key=api_key)
            model = client.models.GenerativeModel(
                model_name=MODEL_NAME, 
                safety_settings=safety_settings_list
            )
            
            # 2. Upload
            print(f"[THREAD] Tentando UP/GEN/DEL com {key_name} para {os.path.basename(file_path)}")
            time.sleep(0.5) 
            
            # Upload usa o cliente espec√≠fico da chave atual
            uploaded_file = client.files.upload(file=Path(file_path)) 
            
            # 3. Gera√ß√£o de Conte√∫do
            prompt_payload = [
                f"1 arquivo anexado ({os.path.basename(file_path)}).",
                PROMPT_TEXT,
                uploaded_file
            ]
            
            config = GenerateContentConfig(
                safety_settings=safety_settings_list
            )
            
            # A gera√ß√£o usa o modelo espec√≠fico da chave atual
            response = model.generate_content(
                contents=prompt_payload,
                config=config, 
            )
            
            # 4. Parsing e Sucesso
            df = parse_markdown_table(response.text)
            if df is not None:
                print(f"[THREAD] SUCESSO na convers√£o para DataFrame de {os.path.basename(file_path)} com {key_name}.")
                return df
            else:
                print(f"[THREAD] Falha de convers√£o: {os.path.basename(file_path)}. Tentar pr√≥xima chave.")
                continue # Tenta a pr√≥xima chave se a convers√£o falhar

        except APIError as e:
            if "RESOURCE_EXHAUSTED" in str(e) or "429" in str(e):
                print(f"[THREAD] ERRO de COTA (429 RESOURCE_EXHAUSTED) com {key_name}.")
                retry_delay = 15 
                match = re.search(r"'retryDelay': '(\d+)s'", str(e))
                if match:
                    retry_delay = int(match.group(1)) + 1 
                
                print(f"[THREAD] Aguardando {retry_delay} segundos antes de tentar a pr√≥xima chave...")
                time.sleep(retry_delay) 
                # Continua o loop para tentar a pr√≥xima chave
            
            elif "PERMISSION_DENIED" in str(e) or "403" in str(e):
                # Este erro pode ocorrer se houver falha no upload/acesso ao arquivo. 
                # √â crucial tentar a pr√≥xima chave.
                print(f"[THREAD] ERRO FATAL (403 PERMISSION_DENIED) com {key_name}: Arquivo n√£o pode ser acessado. Tentando pr√≥xima chave.")
                time.sleep(5) # Espera um pouco antes de tentar o pr√≥ximo cliente
            
            else:
                print(f"[THREAD] ERRO INESPERADO da API com {key_name}: {e}. Tentando pr√≥xima chave.")
                time.sleep(5)
            
            continue # Tenta a pr√≥xima chave

        except Exception as e:
            print(f"[THREAD] ERRO geral (Upload ou Conex√£o) com {key_name} para {os.path.basename(file_path)}: {e}. Tentando pr√≥xima chave.")
            time.sleep(5)
            continue # Tenta a pr√≥xima chave
        
        finally:
            if uploaded_file:
                # 5. Dele√ß√£o (CR√çTICO: Usa o cliente que FEZ o upload, que √© o 'client' atual)
                print(f"[THREAD] Limpando arquivo {uploaded_file.name} do servidor Gemini...")
                try:
                    time.sleep(0.5) 
                    client.files.delete(name=uploaded_file.name)
                    uploaded_file = None # Reseta o arquivo upado para a pr√≥xima tentativa
                except Exception as e:
                    # Se o erro for 403, pode ser que o upload n√£o tenha funcionado, ou o delete falhou.
                    print(f"[THREAD] Erro ao deletar {uploaded_file.name} com {key_name}: {e}")
                    # N√£o levantamos exce√ß√£o aqui, pois a tarefa j√° falhou ou teve sucesso.
    
    # Se o loop terminar sem sucesso
    print(f"[THREAD] FALHA TOTAL: N√£o foi poss√≠vel processar {os.path.basename(file_path)} ap√≥s {len(API_KEY_LIST)} tentativas de failover.")
    return None

def process_files():
    print(f"Procurando por arquivos .zip em {artifact_folder}...")
    zip_pattern = os.path.join(artifact_folder, "**", "*.zip")
    zip_files = glob.glob(zip_pattern, recursive=True)

    # ... (L√≥gica de Extra√ß√£o de Zips inalterada) ...
    if not zip_files:
        print("Nenhum arquivo .zip encontrado. Verificando arquivos existentes...")
    else:
        print(f"Encontrados {len(zip_files)} arquivos .zip. Extraindo...")
        for zip_path in zip_files:
            try:
                extract_directory = os.path.dirname(zip_path)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_directory)
                print(f"Extra√≠do: {zip_path} -> {extract_directory}")
            except zipfile.BadZipFile:
                print(f"Erro: {zip_path} n√£o √© um arquivo zip v√°lido ou est√° corrompido.")
            except Exception as e:
                print(f"Erro ao extrair {zip_path}: {e}")
        print("Extra√ß√£o de Zips conclu√≠da.\n")

    print("Iniciando varredura das pastas de supermercados...")
    all_file_paths = []
    
    for root, dirs, files in os.walk(artifact_folder, topdown=False):
        if not dirs and files and root != artifact_folder:
            file_paths_to_process = [
                os.path.join(root, f) for f in files if f.lower().endswith(VALID_EXTENSIONS)
            ]
            all_file_paths.extend(file_paths_to_process)

    if not all_file_paths:
        print("Nenhum arquivo v√°lido encontrado para processamento.")
        return

    print(f"TOTAL: {len(all_file_paths)} arquivos encontrados para processar.")
    print(f"Processando em paralelo com at√© {MAX_THREADS} threads...")

    # üí° MUDAN√áA CR√çTICA: Criar um iterador c√≠clico de chaves para distribui√ß√£o inicial
    key_cycle = cycle(API_KEY_LIST)
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # Passa o iterador c√≠clico para cada thread, garantindo rota√ß√£o
        future_to_file = {executor.submit(process_single_file, path, key_cycle): path for path in all_file_paths}
        
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                df_result = future.result() 
                if df_result is not None:
                    all_dataframes.append(df_result) 
                
            except Exception as exc:
                print(f"Arquivo {os.path.basename(file_path)} gerou uma exce√ß√£o: {exc}")

    
    if not all_dataframes:
        print("Nenhum resultado foi gerado pela API ou convertido para DataFrame.")
    else:
        save_dataframes_to_excel(all_dataframes)


if __name__ == "__main__":
    try:
        # ... (Verifica√ß√£o de depend√™ncias inalterada) ...
        import pandas as pd
        import openpyxl 
    except ImportError:
        print("\n--- DEPEND√äNCIA FALTANDO ---")
        print("Para salvar em XLSX, voc√™ precisa instalar pandas e openpyxl.")
        print("Execute o comando:")
        print("pip install pandas openpyxl")
        exit()

    try:
        process_files()
    except Exception as e:
        print(f"Um erro inesperado e fatal ocorreu: {e}")